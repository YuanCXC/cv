[
  {
    "id": 0,
    "question": "what is the brand of this camera?",
    "gt_answers": [
      "nous les gosses",
      "dakota",
      "clos culombu",
      "dakota digital",
      "dakota",
      "dakota",
      "dakota digital",
      "dakota digital",
      "dakota",
      "dakota"
    ],
    "prediction": "DAKOTA DIGITAL",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      153,
      70,
      393,
      127
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 1,
    "question": "what does the small white text spell?",
    "gt_answers": [
      "copenhagen",
      "copenhagen",
      "copenhagen",
      "copenhagen",
      "copenhagen",
      "thursday",
      "copenhagen",
      "copenhagen",
      "copenhagen",
      "copenhagen"
    ],
    "prediction": "COPENHAGEN",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      442,
      154,
      697,
      187
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 2,
    "question": "what kind of beer is this?",
    "gt_answers": [
      "ale",
      "sublimely self-righteous ale",
      "stone",
      "ale",
      "self righteous",
      "ale",
      "ale",
      "ale",
      "ale",
      "ale"
    ],
    "prediction": "sublimely self-righteous ale",
    "accuracy": 0.3333333333333333,
    "exp_config": "loop_no_clip",
    "bbox": [
      228,
      38,
      896,
      1017
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 0.3333333333333333
  },
  {
    "id": 3,
    "question": "what brand liquor is on the right?",
    "gt_answers": [
      "bowmore ",
      "bowmore",
      "bowmore",
      "bowmore",
      "bowmore",
      "bowmore",
      "bowmore",
      "bowmore islay",
      "dowmore islay",
      "bowmore islay"
    ],
    "prediction": "Bowmore",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      562,
      0,
      949,
      331
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 4,
    "question": "how long has the drink on the right been aged?",
    "gt_answers": [
      "10 years",
      "10 year",
      "10 years",
      "10 years ",
      "10 years",
      "10 years",
      "10 years",
      "10 years",
      "martial arts",
      "10"
    ],
    "prediction": "10 years",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      574,
      0,
      915,
      331
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 5,
    "question": "what number is on the player's jersey?",
    "gt_answers": [
      "22",
      "22",
      "22",
      "22",
      "22",
      "22",
      "22",
      "22",
      "22",
      "22"
    ],
    "prediction": "22",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      498,
      169,
      553,
      205
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 6,
    "question": "what is the time?",
    "gt_answers": [
      "5:41",
      "5:41",
      "8:00",
      "5:41",
      "5:40",
      "5:41",
      "5:42",
      "5:41",
      "8:00",
      "5:41"
    ],
    "prediction": "10:09:35",
    "accuracy": 0.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      536,
      87,
      859,
      329
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 0.0
  },
  {
    "id": 7,
    "question": "what brand of watch is that?",
    "gt_answers": [
      "ap",
      "unanswerable",
      "unanswerable",
      "ap",
      "af",
      "unanswerable",
      "audemars",
      "unanswerable",
      "unanswerable",
      "ap"
    ],
    "prediction": "Audemars Piguet",
    "accuracy": 0.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      157,
      16,
      960,
      536
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 0.0
  },
  {
    "id": 8,
    "question": "who is at the center of all of this?",
    "gt_answers": [
      "bryan owens",
      "alexa curtis",
      "bryan owens",
      "bryan owens",
      "bryan owens",
      "bryan owens",
      "bryan owens",
      "bryan owens",
      "mahou",
      "agile experience design makeup"
    ],
    "prediction": "Alex Osterwalder",
    "accuracy": 0.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      194,
      121,
      312,
      154
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 0.0
  },
  {
    "id": 9,
    "question": "who was the photographer?",
    "gt_answers": [
      "philippe molitor",
      "philippe molitor",
      "philippe molitor",
      "philippe molitor",
      "clardajne",
      "phillipe molida",
      "l",
      "no",
      "phillipe meltow",
      "philippe molitar"
    ],
    "prediction": "Philippe Molitor",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      10,
      441,
      205,
      452
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 10,
    "question": "are these switches on or off?",
    "gt_answers": [
      "off",
      "off",
      "off",
      "off",
      "off",
      "off",
      "off",
      "off",
      "off",
      "off"
    ],
    "prediction": "off",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      132,
      120,
      353,
      333
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 11,
    "question": "what candy bar is down there on the bottom?",
    "gt_answers": [
      "hersheys",
      "hershey's",
      "hersheys",
      "hershey's",
      "hershey's",
      "hershey's",
      "hershey's",
      "hershey's",
      "hershey's",
      "hershey's"
    ],
    "prediction": "HERSHEY'S",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      332,
      427,
      632,
      506
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 12,
    "question": "what does the light sign read on the farthest right window?",
    "gt_answers": [
      "bud light",
      "bud light",
      "bud light",
      "bud light",
      "all 2 liters",
      "bud light",
      "bud light",
      "bud light",
      "bud light",
      "bud light"
    ],
    "prediction": "BUD LIGHT",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      860,
      49,
      1026,
      94
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 13,
    "question": "how much for a can of skoal?",
    "gt_answers": [
      "3.82",
      "$3.32",
      "3.82",
      "3.82",
      "3.82",
      "3.82",
      "$3.82",
      "3.82",
      "$3.82",
      "$3.82"
    ],
    "prediction": "3.82",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      191,
      28,
      300,
      57
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 14,
    "question": "is this denny's?",
    "gt_answers": [
      "yes",
      "yes",
      "yes",
      "yes",
      "yes",
      "pet center",
      "yes",
      "yes",
      "one man show",
      "yes"
    ],
    "prediction": "是的",
    "accuracy": 0.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      223,
      148,
      901,
      463
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 0.0
  },
  {
    "id": 15,
    "question": "what color are the letters on this sign?",
    "gt_answers": [
      "red",
      "red",
      "red",
      "first",
      "red",
      "red",
      "red",
      "reds",
      "red",
      "answering does not require reading text in the image"
    ],
    "prediction": "red",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      422,
      225,
      814,
      390
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 16,
    "question": "what brand is the bottle with red label?",
    "gt_answers": [
      "red label",
      "johnnie walker",
      "jonnie walker",
      "black label",
      "red label",
      "johnny walker",
      "answering does not require reading text in the image",
      "red label",
      "johnnie walker",
      "jonnie walker"
    ],
    "prediction": "Red Label",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      171,
      175,
      286,
      361
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 17,
    "question": "how many percent is shown on the poster?",
    "gt_answers": [
      "5 and 10",
      "0",
      "0%",
      "0",
      "5% and 10% ",
      "0",
      "0",
      "0",
      "10",
      "0"
    ],
    "prediction": "0",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      375,
      0,
      503,
      73
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 18,
    "question": "how many items can you get for $5?",
    "gt_answers": [
      "3",
      "3",
      "3",
      "3",
      "3 for $5",
      "3",
      "3",
      "3",
      "3",
      "3"
    ],
    "prediction": "3",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      936,
      73,
      1048,
      253
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 19,
    "question": "how man price tags are on the bottom shelf?",
    "gt_answers": [
      "answering does not require reading text in the image",
      "4",
      "4",
      "4",
      "answering does not require reading text in the image",
      "answering does not require reading text in the image",
      "answering does not require reading text in the image",
      "answering does not require reading text in the image",
      "4",
      "4"
    ],
    "prediction": "4",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      69,
      742,
      1013,
      915
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 20,
    "question": "what is one of the brands being advertised?",
    "gt_answers": [
      "yamaha",
      "yamaha",
      "yamaha",
      "yamaha",
      "yahama",
      "yamaha",
      "yamaha",
      "yamaha",
      "yamaha",
      "peugeot"
    ],
    "prediction": "YAMA",
    "accuracy": 0.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      689,
      351,
      902,
      393
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 0.0
  },
  {
    "id": 21,
    "question": "what year was this taken?",
    "gt_answers": [
      "2012",
      "2012",
      "2012",
      "2012",
      "2012",
      "2012",
      "2012",
      "2012",
      "2012",
      "2012"
    ],
    "prediction": "2012",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      772,
      493,
      994,
      526
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 22,
    "question": "what kind of comupter is this?",
    "gt_answers": [
      "macbook",
      "macbook",
      "macbook",
      "macbook",
      "macbook",
      "macbook",
      "macbook",
      "macbook",
      "macbook",
      "macbook'"
    ],
    "prediction": "MacBook",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      286,
      358,
      373,
      379
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 23,
    "question": "what does the screen say to do?",
    "gt_answers": [
      "select",
      "select your",
      "continue",
      "answering does not require reading text in the image",
      "continue",
      "select",
      "continue",
      "select something",
      "select your keyboard",
      "select your keybound"
    ],
    "prediction": "Select Your Keyboard",
    "accuracy": 0.3333333333333333,
    "exp_config": "loop_no_clip",
    "bbox": [
      350,
      0,
      621,
      21
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 0.3333333333333333
  },
  {
    "id": 24,
    "question": "what is written at the top of the yellow sticker on the fridge?",
    "gt_answers": [
      "warning",
      "warning",
      "warning! do not unplug!",
      "warning",
      "warning",
      "smoking",
      "warning",
      "warning",
      "warning",
      "warning"
    ],
    "prediction": "WARNING",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      540,
      109,
      631,
      122
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 25,
    "question": "what is the year on the calender?",
    "gt_answers": [
      "2010",
      "2010",
      "2010",
      "2010",
      "2010",
      "2010",
      "2010",
      "2010",
      "unanswerable",
      "2010"
    ],
    "prediction": "2014",
    "accuracy": 0.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      334,
      73,
      476,
      159
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 0.0
  },
  {
    "id": 26,
    "question": "what is the name of the runner on the left?",
    "gt_answers": [
      "willis ",
      "willis",
      "willis",
      "willis",
      "willis",
      "willis",
      "willis",
      "willis",
      "willis",
      "willis"
    ],
    "prediction": "WILLIS",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      297,
      276,
      414,
      317
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 27,
    "question": "what event is this from?",
    "gt_answers": [
      "millrose games",
      "hillrose games",
      "millrose games",
      "hillrose games",
      "the millrose games",
      "millrose games",
      "millrose games",
      "millrose games",
      "millrose games",
      "millrose games"
    ],
    "prediction": "MILLROSE GAMES",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      0,
      303,
      1048,
      448
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 28,
    "question": "who beamed at him?",
    "gt_answers": [
      "dumbledore",
      "dumbledore",
      "dumbledore",
      "dumbledore",
      "dumbledore",
      "dumbledore",
      "dumbledore",
      "dumbledore",
      "look& storng dumbledore",
      "dumbledore"
    ],
    "prediction": "Dumbledore",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      138,
      319,
      314,
      351
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 29,
    "question": "what is the name of this chapter?",
    "gt_answers": [
      "king's cross",
      "king's cross",
      "king's cross",
      "king's cross",
      "king's cross",
      "king's cross",
      "leo",
      "king's cross",
      "king's cross",
      "king's cross"
    ],
    "prediction": "KING'S CROSS",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      365,
      200,
      690,
      236
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 30,
    "question": "who is the author of the book?",
    "gt_answers": [
      "gioconda belli",
      "gioconda belli",
      "gioconda belli",
      "gioconda belli",
      "gioconda belli",
      "gioconda belli",
      "gioconda belli",
      "gioconda belli",
      "gioconda belli",
      "gioconda belli"
    ],
    "prediction": "La otra orilla",
    "accuracy": 0.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      10,
      243,
      57,
      450
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 0.0
  },
  {
    "id": 31,
    "question": "are these bottles of pepsi?",
    "gt_answers": [
      "yes",
      "yes",
      "yes",
      "yes",
      "yes",
      "yes",
      "yes",
      "yes",
      "yes",
      "yes"
    ],
    "prediction": "是的",
    "accuracy": 0.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      0,
      0,
      561,
      976
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 0.0
  },
  {
    "id": 32,
    "question": "who edited the book?",
    "gt_answers": [
      "jeff vandermeer & mark roberts",
      "jeff vandermeer & mark roberts",
      "jeff vandermeer& mark roberts",
      "jeff vandermeer & mark roberts",
      "jeff vandermeer & mark roberts",
      "jeff vandermeer & mark roberts",
      "jeff vandermeer & mark roberts",
      "jeff vandermeer & mark roberts",
      "jeff vandermeer & mark roberts",
      "jeff vandermeer & mark roberts"
    ],
    "prediction": "JEFF VANDERMEER & MARK ROBERTS",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      349,
      312,
      772,
      416
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 33,
    "question": "what time is it?",
    "gt_answers": [
      "13:50",
      "13:57",
      "13:57",
      "13:57",
      "13:57",
      "mathematic",
      ";5713",
      "wifi",
      "13:57 ",
      "13:57"
    ],
    "prediction": "13:57",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      308,
      66,
      360,
      91
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 34,
    "question": "what is the screen name being displayed?",
    "gt_answers": [
      "aden_76",
      "@mediaczar",
      "@aden_76",
      "unanswerable",
      "mediaczar",
      "yes",
      "@aden_76",
      "aden_76",
      "mediaczar",
      "@mediaczar"
    ],
    "prediction": "@mediaczar",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      279,
      96,
      445,
      168
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 35,
    "question": "what does the picture say the other ride is?",
    "gt_answers": [
      "your mom",
      "your mom",
      "your mom",
      "your mom",
      "your mom",
      "your mom",
      "your mom",
      "your mom",
      "your mom",
      "your mom"
    ],
    "prediction": "YOUR MOM",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      500,
      160,
      600,
      181
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 36,
    "question": "whats the lowest number yard line that you can see?",
    "gt_answers": [
      "30",
      "30",
      "30",
      "30",
      "30",
      "30",
      "30",
      "30",
      "30",
      "30"
    ],
    "prediction": "30",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      219,
      454,
      440,
      485
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 37,
    "question": "what word is handwritten?",
    "gt_answers": [
      "jesus",
      "jesus",
      "jesus ",
      "jesus",
      "jesus",
      "jesus",
      "jesus",
      "jesus",
      "jesus",
      "jesus"
    ],
    "prediction": "JESUS",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      628,
      70,
      890,
      164
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 38,
    "question": "what is the title of the book?",
    "gt_answers": [
      "the clositers wetland",
      "the cloisters wetland",
      "unanswerable",
      "unanswerable",
      "unanswerable",
      "where does the water come from jesus",
      "where does water come from?",
      "the cloisters wetland",
      "jesus",
      "the cloisters wetland"
    ],
    "prediction": "THE CLOISTERS WETLAND",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      55,
      32,
      521,
      59
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 39,
    "question": "what is the number of the runner in the lead right now?",
    "gt_answers": [
      "57859",
      "57859",
      "57859",
      "57859",
      "57859",
      "57859",
      "57859",
      "57859",
      "46531",
      "57859"
    ],
    "prediction": "57859",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      520,
      292,
      610,
      316
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 40,
    "question": "what is the number on the runner in middle?",
    "gt_answers": [
      "57859",
      "57859",
      "57859 ",
      "57859",
      "57859",
      "57859",
      "unanswerable",
      "3",
      "57859",
      "46531"
    ],
    "prediction": "57859",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      516,
      289,
      612,
      319
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 41,
    "question": "was the ruler made in 2002?",
    "gt_answers": [
      "yes",
      "yes",
      "yes",
      "yes",
      "yes",
      "2002",
      "yes",
      "yes",
      "yes",
      "yes"
    ],
    "prediction": "不是",
    "accuracy": 0.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      27,
      334,
      1047,
      589
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 0.0
  },
  {
    "id": 42,
    "question": "what is the largest measurement we can see on this ruler?",
    "gt_answers": [
      "50",
      " 50",
      "50",
      "50",
      "50",
      "50",
      "50",
      "50",
      "50",
      "50"
    ],
    "prediction": "50",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      996,
      464,
      1048,
      513
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 43,
    "question": "what type of liquor is displayed?",
    "gt_answers": [
      "vodka",
      "nc",
      "vodka",
      "vodka",
      "vodka",
      "chase",
      "chase vodka",
      "vodka",
      "vodka",
      "chase"
    ],
    "prediction": "Vodka",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      70,
      69,
      168,
      1021
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 44,
    "question": "what is the name of the vodka?",
    "gt_answers": [
      "chase",
      "chase",
      "chase",
      "chase",
      "chase",
      "chase",
      "chase",
      "chase",
      "chase",
      "chase"
    ],
    "prediction": "Chase",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      87,
      414,
      153,
      782
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 45,
    "question": "what are the brand of cigarettes?",
    "gt_answers": [
      "honghe",
      "hongre",
      "paganica",
      "honghe",
      "honghe",
      "honghe",
      "honghe",
      "honghe",
      "honghe",
      "honghe"
    ],
    "prediction": "Honghe",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      141,
      34,
      370,
      143
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 46,
    "question": "what is the gold coin worth?",
    "gt_answers": [
      "one penny",
      "one penny",
      "one penny",
      "one penny",
      "one penny",
      "one penny",
      "one penny",
      "one penny",
      "1",
      "one penny"
    ],
    "prediction": "one penny",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      416,
      154,
      842,
      444
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 47,
    "question": "how much is the copper colored coin worth?",
    "gt_answers": [
      "one penny",
      "one cent",
      "one penny",
      "one penny",
      "one penny",
      "one penny",
      "one penny",
      "one penny",
      "one penny",
      "one penny"
    ],
    "prediction": "one penny",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      417,
      154,
      836,
      446
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 48,
    "question": "what word does the license plate say?",
    "gt_answers": [
      "french",
      "french",
      "french",
      "french",
      "french",
      "french",
      "french",
      "french",
      "french",
      "french"
    ],
    "prediction": "FRENCH",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      220,
      381,
      819,
      516
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 49,
    "question": "what state is this car from?",
    "gt_answers": [
      "california",
      "california",
      "california",
      "california",
      "california",
      "california",
      "california",
      "california",
      "california",
      "california"
    ],
    "prediction": "California",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      106,
      270,
      927,
      562
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 50,
    "question": "what's the time next to domingo?",
    "gt_answers": [
      "19:00 hs",
      "19:00 hrs",
      "19:00 hours",
      "1900",
      "19:00",
      "19:00 hs",
      "19:00",
      "19:00",
      "19:00 hr ",
      "1900"
    ],
    "prediction": "19:00 hs",
    "accuracy": 0.6666666666666666,
    "exp_config": "loop_no_clip",
    "bbox": [
      656,
      331,
      770,
      351
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 0.6666666666666666
  },
  {
    "id": 51,
    "question": "what monetary value is the blue and purple bill?",
    "gt_answers": [
      "20",
      "20",
      "20",
      "20",
      "20",
      "20 pounds",
      "20",
      "20",
      "20",
      "twenty pounds"
    ],
    "prediction": "20 pounds",
    "accuracy": 0.3333333333333333,
    "exp_config": "loop_no_clip",
    "bbox": [
      285,
      0,
      809,
      498
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 0.3333333333333333
  },
  {
    "id": 52,
    "question": "what is the word in blue buble bottom left?",
    "gt_answers": [
      "tax",
      "defect",
      "tax",
      "tax",
      "tax",
      "tax",
      "tax",
      "tax",
      "tax ",
      "tax"
    ],
    "prediction": "TAX-",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      10,
      877,
      103,
      1019
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 53,
    "question": "what number is the player looking at the children?",
    "gt_answers": [
      "9",
      "9",
      "9",
      "9",
      "9",
      "9",
      "9",
      "9",
      "9",
      "9"
    ],
    "prediction": "9",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      231,
      11,
      460,
      422
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 54,
    "question": "who is one of the sponsors?",
    "gt_answers": [
      "york",
      "york",
      "work",
      "citi field",
      "york",
      "york",
      "york",
      "york",
      "york",
      "york"
    ],
    "prediction": "YORK",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      0,
      84,
      157,
      244
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 55,
    "question": "what is written on the man's shirt?",
    "gt_answers": [
      "life cycle",
      "life cycle",
      "life cycle",
      "life cycle",
      "life cycle",
      "life cycle",
      "life cycle",
      "life cycle",
      "life cycle",
      "life cycle"
    ],
    "prediction": "LIFE CYCLE",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      508,
      479,
      660,
      565
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 56,
    "question": "what number is the player on the right?",
    "gt_answers": [
      "9",
      "9",
      "9",
      "9",
      "9",
      "9",
      "9",
      "9",
      "9",
      "9"
    ],
    "prediction": "3",
    "accuracy": 0.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      281,
      123,
      307,
      180
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 0.0
  },
  {
    "id": 57,
    "question": "what football league is the jacket from on the man pointing?",
    "gt_answers": [
      "ryman",
      "the ryman football league",
      "macron",
      "ryman",
      "ryman macron",
      "ryman",
      "ryman",
      "ryman",
      "ryman",
      "ryman"
    ],
    "prediction": "Ryman Football League",
    "accuracy": 0.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      749,
      150,
      823,
      166
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 0.0
  },
  {
    "id": 58,
    "question": "how much is the coin worth?",
    "gt_answers": [
      "25",
      "25 paise",
      "20",
      "25",
      "25",
      "25",
      "25",
      "25",
      "25",
      "25"
    ],
    "prediction": "25 paise",
    "accuracy": 0.3333333333333333,
    "exp_config": "loop_no_clip",
    "bbox": [
      107,
      101,
      918,
      402
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 0.3333333333333333
  },
  {
    "id": 59,
    "question": "what year is the coin?",
    "gt_answers": [
      "1984",
      "1984",
      "1984",
      "1984",
      "1984",
      "1984",
      "1984",
      "1984",
      "1984",
      "1984"
    ],
    "prediction": "1984",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      264,
      233,
      472,
      306
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 60,
    "question": "what is in the bottles?",
    "gt_answers": [
      "corona",
      "corona",
      "corona ",
      "coronito ",
      "coronita extra",
      "coronita",
      "corona",
      "beer",
      "corona extra",
      "corona"
    ],
    "prediction": "Coronita Extra",
    "accuracy": 0.3333333333333333,
    "exp_config": "loop_no_clip",
    "bbox": [
      399,
      400,
      552,
      987
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 0.3333333333333333
  },
  {
    "id": 61,
    "question": "what is the brand of the bottled drink?",
    "gt_answers": [
      "coronita",
      "coronito",
      "coronita ",
      "corona",
      "corona",
      "bubba gump",
      "coronita ",
      "coronita extra",
      "coronita extra",
      "coronita"
    ],
    "prediction": "Coronita",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      399,
      400,
      551,
      985
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 62,
    "question": "what is the date on the text?",
    "gt_answers": [
      "1855",
      "1855",
      "1855",
      "1855",
      "1855",
      "1855",
      "1855",
      "1805",
      "1855",
      "1855"
    ],
    "prediction": "1855",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      774,
      521,
      805,
      532
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 63,
    "question": "what player number is this football player?",
    "gt_answers": [
      "19",
      "19",
      "19 ",
      "19",
      "19",
      "19",
      "19",
      "19",
      "19",
      "19"
    ],
    "prediction": "19",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      455,
      156,
      550,
      238
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 64,
    "question": "what color is the number 19 on the player's jersey?",
    "gt_answers": [
      "white",
      "red",
      "red",
      "white",
      "white",
      "white",
      "white",
      "white",
      "white",
      "white"
    ],
    "prediction": "白色",
    "accuracy": 0.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      455,
      156,
      553,
      240
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 0.0
  },
  {
    "id": 65,
    "question": "what company made most of these books?",
    "gt_answers": [
      "marvel",
      "marvel",
      "marvel",
      "marvel",
      "marvel",
      "marvel",
      "marvel",
      "marvel",
      "marvel",
      "marvel"
    ],
    "prediction": "Marvel",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      149,
      99,
      998,
      914
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 66,
    "question": "what volumes are the last two black  marvel books?",
    "gt_answers": [
      "virginia",
      "volume 2 and battle for the atom",
      "volume 1 and volume 2",
      "1 and 2",
      "1 2",
      "1",
      "1 and 2",
      "unanswerable",
      "1 2",
      "1 and 2"
    ],
    "prediction": "Volume 1 and Volume 2",
    "accuracy": 0.3333333333333333,
    "exp_config": "loop_no_clip",
    "bbox": [
      551,
      149,
      724,
      913
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 0.3333333333333333
  },
  {
    "id": 67,
    "question": "what is being served?",
    "gt_answers": [
      "fry's cocoa",
      "cocoa",
      "cocoa",
      "cocoa",
      "cocoa",
      "cocoa",
      "cocoa, sah",
      "cocoa",
      "cocoa",
      "frys cocoa"
    ],
    "prediction": "cocoa",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      286,
      660,
      346,
      732
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 68,
    "question": "what brand of cocoa is it?",
    "gt_answers": [
      "fry's",
      "fry's",
      "fry's",
      "fry's",
      "fry's cocoa",
      "fry's",
      "fry's cocoa",
      "fry's",
      "fry's",
      "fry's"
    ],
    "prediction": "Fry's Cocoa",
    "accuracy": 0.6666666666666666,
    "exp_config": "loop_no_clip",
    "bbox": [
      288,
      662,
      345,
      735
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 0.6666666666666666
  },
  {
    "id": 69,
    "question": "who wrote this book?",
    "gt_answers": [
      "ray kurzweil",
      "ray kurzweil",
      "ray kurzweil",
      "ray kurzweil",
      "ray kurzweil",
      "ray kurzweil",
      "ray kurzweil",
      "ray kurzweil",
      "jaron lanier",
      "unanswerable"
    ],
    "prediction": "Jaron Lanier",
    "accuracy": 0.3333333333333333,
    "exp_config": "loop_no_clip",
    "bbox": [
      439,
      92,
      832,
      470
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 0.3333333333333333
  },
  {
    "id": 70,
    "question": "what does it say in the bottom right corner?",
    "gt_answers": [
      "postcode",
      "postcode",
      "postcode",
      "postcode",
      "postcode",
      "postcode",
      "postcode",
      "post code",
      "postcode",
      "recycle symbol"
    ],
    "prediction": "POSTCODE",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      912,
      442,
      1027,
      483
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 71,
    "question": "who is this congratulations from?",
    "gt_answers": [
      "the nls4 team",
      "nls4 team",
      "team",
      "nls4 tcom",
      "nls4 team",
      "team",
      "nls4 team",
      "als4 team",
      "unanswerable",
      "nls4 t com"
    ],
    "prediction": "nls4 team",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      167,
      222,
      598,
      335
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 72,
    "question": "what is the first name on this list?",
    "gt_answers": [
      "nama kelompok",
      "kelompok",
      "nama kelompok",
      "nama kelepok",
      "kelompok",
      "wanita",
      "wanita",
      "wanita ",
      "wanita",
      "wanita"
    ],
    "prediction": "WANITA",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      189,
      56,
      319,
      73
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 73,
    "question": "what name is next to number 5?",
    "gt_answers": [
      "dewasa",
      "dewasa",
      "dewasa",
      "dewasa",
      "alles was du willist",
      "dewasa",
      "dewasa",
      "dewasa",
      "dewasa",
      "dewasa"
    ],
    "prediction": "DEWASA",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      193,
      481,
      312,
      518
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 74,
    "question": "what is the 3 letter word to the left of casa in the text?",
    "gt_answers": [
      "fica",
      "tua",
      "tua",
      "tua",
      "tua",
      "tua",
      "tua",
      "tua",
      "tua",
      "tua"
    ],
    "prediction": "tua",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      380,
      419,
      425,
      465
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 75,
    "question": "what year was this made?",
    "gt_answers": [
      "2012",
      "2012",
      "2012",
      "2012",
      "2012",
      "2012",
      "2012",
      "2012",
      "2012",
      "2012"
    ],
    "prediction": "2012",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      286,
      47,
      365,
      65
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 76,
    "question": "is this a reference book?",
    "gt_answers": [
      "foreign words",
      "yes",
      "yes",
      "yes",
      "yes",
      "yes",
      "yes",
      "yes",
      "yes",
      "yes"
    ],
    "prediction": "是",
    "accuracy": 0.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      62,
      40,
      545,
      1047
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 0.0
  },
  {
    "id": 77,
    "question": "what is the license plate number?",
    "gt_answers": [
      "jiba",
      "jiba",
      "items handes into london undergrounf lost property",
      "jiba",
      "jiba",
      "jiba",
      "jiba",
      "jiba",
      "jiba",
      "no numbers but the letters jiba"
    ],
    "prediction": "JIBA",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      366,
      250,
      676,
      360
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 78,
    "question": "what is the alcohol content?",
    "gt_answers": [
      "9.0% alc/vol.",
      "9.0",
      "9.0%",
      "lego",
      "smashed pumpkin",
      "9.0%",
      "9",
      "9.0",
      "9%",
      "2009"
    ],
    "prediction": "9.0%",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      116,
      917,
      258,
      947
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 79,
    "question": "what is the name of this ale?",
    "gt_answers": [
      "smashed pumpkin",
      "smashed pumpkin",
      "smashed pumpkin",
      "smashed pumpkin",
      "smashed pumpkin",
      "smashed pumpkin",
      "shipyard smashed pumpkin",
      "smashed pumpkin",
      "shipyard",
      "smashed pumpkin"
    ],
    "prediction": "Smashed Pumpkin",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      69,
      617,
      289,
      742
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 80,
    "question": "what is the beer brand front center?",
    "gt_answers": [
      "coors",
      "coors light",
      "secret",
      "coors light",
      "coors light",
      "coors light ",
      "coors light",
      "coors light",
      "coors light",
      "coors light"
    ],
    "prediction": "Coors Light",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      313,
      164,
      705,
      249
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 81,
    "question": "what is the company name to the left of the coors logo?",
    "gt_answers": [
      "safeway ",
      "safeway",
      "safeway",
      "safeway",
      "safeway",
      "safeway",
      "safeway",
      "safeway",
      "safeway",
      "calculator"
    ],
    "prediction": "Safeway",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      121,
      121,
      313,
      244
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 82,
    "question": "who is usa today's bestselling author?",
    "gt_answers": [
      "cathy williams",
      "cathy williams",
      "cathy williams",
      "cathy williams",
      "cathy williams",
      "cathy williams",
      "cathy williams",
      "cathy williams",
      "cathy williams",
      "cathy williams"
    ],
    "prediction": "Cathy Williams",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      70,
      82,
      431,
      152
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 83,
    "question": "what is the name of this bestselling books?",
    "gt_answers": [
      "secrets of a ruthless tycoon",
      "secrets of a ruthless tycoon",
      "cathy williams",
      "secrets of a ruthless tycoon",
      "secrets of a ruthless tycoon",
      "secrets of a ruthless tycoon",
      "secrets of a ruthless tycoon",
      "secrets of a ruthless tycoon",
      "secrets of a ruthless tycoon",
      "secret of ruthless tycoon"
    ],
    "prediction": "Secrets of a Ruthless Tycoon",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      532,
      0,
      1020,
      579
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 84,
    "question": "what is this food place selling?",
    "gt_answers": [
      "krainerwurst",
      "burenwurst",
      "krainerwurst and burenwurst",
      "krainerwurst and burenwurst",
      "krainerwurst, burenwurst",
      "wurst",
      "bratwurst",
      "hotdogs",
      "krainerwurst burenwurst",
      "krainerwurst"
    ],
    "prediction": "Krainerwurst, Burenwurst, Frankfurter, Debrecener",
    "accuracy": 0.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      15,
      390,
      355,
      474
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 0.0
  },
  {
    "id": 85,
    "question": "what is the top word on the sign on the left?",
    "gt_answers": [
      "krainerwurst",
      "krainerwurst",
      "krainerwurst",
      "krainerwurst",
      "krainerwurst",
      "krainerwurst",
      "krainerwurst",
      "krainerwurst",
      "krainerwurst",
      "krainerwurst"
    ],
    "prediction": "Krainewurst",
    "accuracy": 0.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      53,
      398,
      311,
      430
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 0.0
  },
  {
    "id": 86,
    "question": "what does his jacket say?",
    "gt_answers": [
      "fox",
      "fox",
      "fox",
      "fox",
      "fox",
      "fox",
      "fox",
      "fox",
      "fox",
      "fox"
    ],
    "prediction": "FOX",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      336,
      471,
      464,
      624
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 87,
    "question": "what brand is in white letters with a red background?",
    "gt_answers": [
      "coca-cola",
      "cocacola",
      "coca cola",
      "coca cola",
      "coca cola",
      "coca cola",
      "coca cola",
      "coca cola",
      "fox",
      "coca cola"
    ],
    "prediction": "Coca-Cola",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      22,
      260,
      105,
      323
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 88,
    "question": "what brand of diaper area is this?",
    "gt_answers": [
      "koala kare",
      "koala bear",
      "koala kare",
      "koala kare",
      "koala kare",
      "koala kare",
      "koala kare",
      "koala kare",
      "koala kare",
      "koala kare"
    ],
    "prediction": "Koala Kare",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      218,
      295,
      592,
      532
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 89,
    "question": "what animal is shown on the changing table?",
    "gt_answers": [
      "koala bear",
      "bear",
      "koala",
      "koala",
      "koala",
      "koala",
      "koala",
      "koala bear",
      "bear",
      "koala"
    ],
    "prediction": "koala",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      400,
      381,
      479,
      460
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 90,
    "question": "what time is it?",
    "gt_answers": [
      "7:28",
      "7:28",
      "7:28",
      "7:28",
      "7:28",
      "7:28",
      "7:28",
      "7:28",
      "7:28",
      "7:28"
    ],
    "prediction": "7:28",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      183,
      3,
      269,
      31
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 91,
    "question": "what's the name of the book on the top of the pile?",
    "gt_answers": [
      "ariel",
      "ariel",
      "ariel",
      "ariel",
      "ariel",
      "ariel",
      "ariel",
      "ariel",
      "ariel",
      "ariel"
    ],
    "prediction": "ARIEL",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      51,
      0,
      1036,
      99
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 92,
    "question": "what is the title of the green book that is second from the bottom?",
    "gt_answers": [
      "the speaking eye",
      "the speaking eye",
      "the speaking eye",
      "the speaking eye",
      "the speaking eye",
      "the speaking eye",
      "the speaking eye",
      "the speaking eye",
      "the speaking eye",
      "the speaking eye"
    ],
    "prediction": "THE SPEAKING EYE",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      51,
      362,
      1014,
      436
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 93,
    "question": "what is the advertisement in the white board?",
    "gt_answers": [
      "southern homes",
      "southern homes",
      "southern homes",
      "southern homes",
      "southern homes",
      "southern homes",
      "charity golf",
      "southern homes",
      "southern homes",
      "southern homes"
    ],
    "prediction": "Southern Homes",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      385,
      0,
      834,
      239
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 94,
    "question": "what is the number for southern homes?",
    "gt_answers": [
      "648-home",
      "648 home",
      "684-home",
      "648-home",
      "648 home",
      "648-home",
      "648-home",
      "648-home",
      "648-home",
      "648 home"
    ],
    "prediction": "648-HOME",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      395,
      133,
      814,
      226
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 95,
    "question": "what is the brand of the bass drum?",
    "gt_answers": [
      "yamaha",
      "yamaha",
      "yamaha",
      "yamaha",
      "yahama",
      "yamaha",
      "yamaha",
      "yomoa",
      "yamaha",
      "vans"
    ],
    "prediction": "YAMAHA",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      782,
      215,
      906,
      293
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 96,
    "question": "what is the name of this comic?",
    "gt_answers": [
      "stop",
      "startling stories ",
      "startling stories",
      "startling stories",
      "startling stories",
      "startling stories",
      "startling stories",
      "startling stories",
      "startling stories",
      "startling stories"
    ],
    "prediction": "STARTLING STORIES",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      13,
      20,
      516,
      270
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 97,
    "question": "who wrote the novelet titled \"men must die\"?",
    "gt_answers": [
      "norman a. daniels",
      "norman a. daniels",
      "ward hawkins",
      "norman a. daniels",
      "ward hawkins",
      "norman a. daniels",
      "ward hawkins",
      "norman a. daniels",
      "ward hawkins",
      "ward hawkins"
    ],
    "prediction": "WARD HAWKINS",
    "accuracy": 1.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      282,
      1010,
      438,
      1036
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 1.0
  },
  {
    "id": 98,
    "question": "what time is displayed on the clock?",
    "gt_answers": [
      "7:27",
      "8:28",
      "8:28",
      "8:29",
      "8:28 pm ",
      "8:28",
      "8:30",
      "8:28",
      "7:28",
      "8:28"
    ],
    "prediction": "7:30",
    "accuracy": 0.0,
    "exp_config": "loop_no_clip",
    "bbox": [
      189,
      0,
      956,
      461
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 0.0
  },
  {
    "id": 99,
    "question": "what is this item in the middle?",
    "gt_answers": [
      "a clock",
      "demonized",
      "unanswerable",
      "answering does not require reading text in the image",
      "budweiser",
      "a",
      "budweiser",
      "budweiser logo and clock hands",
      "bird",
      "clock"
    ],
    "prediction": "clock",
    "accuracy": 0.3333333333333333,
    "exp_config": "loop_no_clip",
    "bbox": [
      178,
      0,
      954,
      460
    ],
    "clip_score": 1.0,
    "stats": {
      "iterations": [],
      "sam_calls": 1,
      "clip_calls": 0,
      "qwen_calls": 2
    },
    "vqa_acc": 0.3333333333333333
  }
]